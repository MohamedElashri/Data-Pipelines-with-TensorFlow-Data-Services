{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTBSvHcSLBzc"
   },
   "outputs": [],
   "source": [
    "# Use all imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from os import getcwd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGsVrzy84WI2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found a different version 3.0.0 of dataset rock_paper_scissors in data_dir /tf/week1/../tmp2. Using currently defined version 1.0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='rock_paper_scissors',\n",
      "    version=1.0.0,\n",
      "    description='Images of hands playing rock, paper, scissor game.',\n",
      "    urls=['http://laurencemoroney.com/rock-paper-scissors-dataset'],\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(300, 300, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
      "    }),\n",
      "    total_num_examples=2892,\n",
      "    splits={\n",
      "        'test': 372,\n",
      "        'train': 2520,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@ONLINE {rps,\n",
      "    author = \"Laurence Moroney\",\n",
      "    title = \"Rock, Paper, Scissors Dataset\",\n",
      "    month = \"feb\",\n",
      "    year = \"2019\",\n",
      "    url = \"http://laurencemoroney.com/rock-paper-scissors-dataset\"\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Use tfds.load to extract the rock_paper_scissors dataset.\n",
    "\n",
    "filePath = f\"{getcwd()}/../tmp2\"\n",
    "data, info = tfds.load(name='rock_paper_scissors', with_info=True, data_dir=filePath)# YOUR CODE HERE (Include the following argument in your code: data_dir=filePath)\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epPGTUqE5Z2E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:372\n",
      "train:2520\n"
     ]
    }
   ],
   "source": [
    "# Write Code that iterates through the splits, without hardcoding any keys\n",
    "# This code should extract 'test' and 'train' as the keys, and then print out\n",
    "# the number of items in the dataset for each key. \n",
    "# HINT: num_examples property is very useful here\n",
    "for key, value in data.items():# YOUR CODE HERE:\n",
    "    print(key + \":\" + str(info.splits[key].num_examples))\n",
    "\n",
    "# EXPECTED OUTPUT\n",
    "# test:372\n",
    "# train:2520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ms5ld5Ov6_OP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Write code that loads the rock_paper_scissors dataset and checks to see if it\n",
    "# supports the new APIs. \n",
    "# HINT: The builder should 'implement' something\n",
    "\n",
    "#rps_builder = tfds.builder(\"rock_paper_scissors:3.*.*\", data_dir=filePath)\n",
    "\n",
    "#print(rps_builder.version.implements(tfds.core.Experiment.S3))\n",
    "\n",
    "rps_builder = rps_builder = tfds.builder(\"rock_paper_scissors:3.*.*\", data_dir=filePath) # YOUR CODE HERE (Include the following arguments in your code: \"rock_paper_scissors:3.*.*\", data_dir=filePath)\n",
    "\n",
    "print(rps_builder.version.implements(tfds.core.Experiment.S3))\n",
    "\n",
    "# Expected output:\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMGkJW6j7Ldl"
   },
   "outputs": [],
   "source": [
    "# Sometimes datasets are too big for prototyping. How would you create\n",
    "# a smaller dataset, where instead of using all of the training data\n",
    "# and all of the test data, you instead have a small_train and small_test\n",
    "# each of which are comprised of the first 10% of the records in the respective\n",
    "# datasets.\n",
    "\n",
    "small_train = tfds.load(\"rock_paper_scissors:3.*.*\", data_dir=filePath, split='train[:10%]') # YOUR CODE HERE (Include the following arguments in your code: \"rock_paper_scissors:3.*.*\", data_dir=filePath)\n",
    "small_test = tfds.load(\"rock_paper_scissors:3.*.*\", data_dir=filePath, split='test[:10%]') # Include the following arguments in your code: \"rock_paper_scissors:3.*.*\", data_dir=filePath)\n",
    "\n",
    "# No expected output yet, that's in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOm99-zO_nAe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# Ok, now let's print out the small versions of the datasets:\n",
    "print(len(list(small_train)))\n",
    "print(len(list(small_test)))\n",
    "\n",
    "# Expected output\n",
    "# 252\n",
    "# 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jL7KXYi17s_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2268\n",
      "335\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "# The original dataset doesn't have a validation dataset, just training and testing\n",
    "# What if you want to use TFDS to create a validation dataset according to these rules\n",
    "# The test set should be the first 90% of the current test set\n",
    "# The training set should be the first 90% of the current training set\n",
    "# The new validation set should be the last 10% of the training set + the last 10% of the test set\n",
    "\n",
    "new_train = tfds.load(\"rock_paper_scissors:3.*.*\", data_dir=filePath, split='train[:90%]') # YOUR CODE HERE (Include the following arguments in your code: \"rock_paper_scissors:3.*.*\", data_dir=filePath)\n",
    "print(len(list(new_train)))\n",
    "\n",
    "new_test = tfds.load(\"rock_paper_scissors:3.*.*\", data_dir=filePath, split='test[:90%]')# YOUR CODE HERE (Include the following arguments in your code: \"rock_paper_scissors:3.*.*\", data_dir=filePath)\n",
    "print(len(list(new_test)))\n",
    "\n",
    "validation = tfds.load(\"rock_paper_scissors:3.*.*\", data_dir=filePath, split='train[-10%:] + test[-10%:]')# YOUR CODE HERE (Include the following arguments in your code: \"rock_paper_scissors:3.*.*\", data_dir=filePath)\n",
    "print(len(list(validation)))\n",
    "\n",
    "# Expected output\n",
    "# 2268\n",
    "# 335\n",
    "# 289"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now click the 'Submit Assignment' button above.\n",
    "# Once that is complete, please run the following two cells to save your work and close the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "<!-- Save the notebook -->\n",
    "IPython.notebook.save_checkpoint();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "<!-- Shutdown and close the notebook -->\n",
    "window.onbeforeunload = null\n",
    "window.close();\n",
    "IPython.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 25 - Exercise - Question.ipynb",
   "provenance": [
    {
     "file_id": "1JCok9fYE1xBsFr0GC-vFa0cUocr-Eo3j",
     "timestamp": 1569508700122
    }
   ]
  },
  "coursera": {
   "course_slug": "data-pipelines-tensorflow",
   "graded_item_id": "kYLnd",
   "launcher_item_id": "YBlDH"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
